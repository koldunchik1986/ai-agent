"""
–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø AI-–ê–°–°–ò–°–¢–ï–ù–¢–ê –î–õ–Ø P104-100 8GB VRAM (sm_61 Pascal)
===============================================================
–í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ—Ç —Ñ–∞–π–ª —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–ª–∞–∂–µ–Ω –¥–ª—è 8GB VRAM. 
–ù–µ –º–µ–Ω—è–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Ö –≤–ª–∏—è–Ω–∏—è –Ω–∞ –ø–∞–º—è—Ç—å!

–û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
1. 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—ç–∫–æ–Ω–æ–º–∏—Ç ~50% VRAM)
2. LoRA –±–µ–∑ lm_head (—ç–∫–æ–Ω–æ–º–∏—Ç ~500MB)
3. batch_size=1 —Å gradient_accumulation (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
4. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π max_new_tokens (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç OOM)
"""

import os
from dataclasses import dataclass, field
from typing import List, Dict
import torch

# ===================================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –ü–ê–ú–Ø–¢–ò
# ===================================================================
# –≠—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –¥–ª—è P104-100 8GB VRAM
# –ü—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ GPU –æ–±–Ω–æ–≤–∏—Ç–µ –∏—Ö —á–µ—Ä–µ–∑ validate_gpu_config()

VRAM_LIMIT_GB = 8.0  # –í—Å–µ–≥–æ VRAM –Ω–∞ P104-100
SAFE_VRAM_USAGE_GB = 6.5  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å)

@dataclass
class ModelConfig:
    """
    –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–ï–õ–ò MISTRAL 7B
    
    –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è 8GB:
    - load_in_8bit=True: –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ! –°–Ω–∏–∂–∞–µ—Ç –ø–∞–º—è—Ç—å —Å 14GB –¥–æ ~7GB
    - load_in_4bit=False: 4-bit –¥–∞–µ—Ç —Ö—É–¥—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º
    - max_new_tokens=1024: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤ (–±–µ—Ä–µ–∂–µ—Ç –ø–∞–º—è—Ç—å)
    - torch_dtype="float16": –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
    """
    
    # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ CodeLlama –∏–ª–∏ DeepSeek Coder)
    model_name: str = "mistralai/Mistral-7B-Instruct-v0.3"
    
    # –ü—É—Ç–∏ –∫ –∫—ç—à—É (–ø–∞–ø–∫–∞ models –≤–Ω—É—Ç—Ä–∏ data)
    cache_dir: str = "/app/data/models"
    
    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda –¥–ª—è GPU, cpu –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    device: str = "cuda"
    
    # –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    torch_dtype: str = "float16"
    
    # ‚úÖ –ö–†–ò–¢–ò–ß–ù–û: 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è 8GB VRAM
    # –ë–µ–∑ —ç—Ç–æ–≥–æ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è (–∑–∞–Ω–∏–º–∞–µ—Ç 14GB –≤ FP16)
    load_in_8bit: bool = True
    
    # 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (–∫–∞—á–µ—Å—Ç–≤–æ —Ö—É–∂–µ, —ç–∫–æ–Ω–æ–º–∏—è –Ω–µ–±–æ–ª—å—à–∞—è)
    load_in_4bit: bool = False
    
    # –î–æ–≤–µ—Ä–∏–µ –∫–æ–¥—É –º–æ–¥–µ–ª–∏ (—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π)
    trust_remote_code: bool = True
    
    # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    use_cache: bool = True
    
    # ‚úÖ –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï: –ú–∞–∫—Å–∏–º—É–º 1024 —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –æ—Ç–≤–µ—Ç
    # –ü—Ä–∏ –±–æ–ª—å—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö –±—É–¥–µ—Ç OOM –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    max_new_tokens: int = 1024
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤)
    temperature: float = 0.7  # 0.3 –¥–ª—è –∫–æ–¥–∞, 0.7 –¥–ª—è –æ–±—â–∏—Ö –∑–∞–¥–∞—á
    top_p: float = 0.9        # Nucleus sampling
    top_k: int = 50           # Top-k sampling
    repetition_penalty: float = 1.1  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—ã
    
    # ‚úÖ –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø IDE –ò–ù–¢–ï–ì–†–ê–¶–ò–ò
    # –ú–µ–Ω—å—à–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –∫–æ–¥—É
    code_temperature: float = 0.3
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
    max_file_tokens: int = 1500  # ~1000-1500 —Ç–æ–∫–µ–Ω–æ–≤ = ~500 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞

@dataclass
class TrainingConfig:
    """
    –ù–ê–°–¢–†–û–ô–ö–ò –î–û–û–ë–£–ß–ï–ù–ò–Ø LORA
    
    –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è 8GB VRAM:
    - per_device_train_batch_size=1: –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ! –ë–æ–ª—å—à–∏–π batch –≤—ã–∑–æ–≤–µ—Ç OOM
    - gradient_accumulation_steps=8: –ö–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π batch
    - lora_r=16: –ú–µ–Ω—å—à–µ = —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ (32 —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ VRAM)
    - optim="paged_adamw_8bit": 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
    """
    
    # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    learning_rate: float = 2e-5
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    num_train_epochs: int = 3
    
    # ‚úÖ –ö–†–ò–¢–ò–ß–ù–û: Batch size = 1 –¥–ª—è 8GB VRAM
    per_device_train_batch_size: int = 1
    
    # Batch size –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    per_device_eval_batch_size: int = 1
    
    # ‚úÖ –ö–û–ú–ü–ï–ù–°–ê–¶–ò–Ø: –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∑–∞ 8 —à–∞–≥–æ–≤
    # –î–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç batch_size=8 –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ª–∏—à–Ω–µ–π –ø–∞–º—è—Ç–∏
    gradient_accumulation_steps: int = 8
    
    # –®–∞–≥–∏ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞
    warmup_steps: int = 100
    
    # –í–µ—Å–æ–≤–æ–π —Ä–∞—Å–ø–∞–¥ (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
    weight_decay: float = 0.01
    
    # –ß–∞—Å—Ç–æ—Ç–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging_steps: int = 10
    
    # –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    save_steps: int = 500
    
    # –ß–∞—Å—Ç–æ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    eval_steps: int = 500
    
    # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (—ç–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞)
    save_total_limit: int = 2
    
    # ‚úÖ –¢–ò–ü –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†–ê: paged_adamw_8bit
    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    optim: str = "paged_adamw_8bit"
    
    # Mixed precision –æ–±—É—á–µ–Ω–∏–µ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ)
    fp16: bool = True
    
    # –¢–∏–ø —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
    lr_scheduler_type: str = "cosine"
    
    # ‚úÖ –ü–ê–†–ê–ú–ï–¢–†–´ LORA
    use_lora: bool = True  # –í–∫–ª—é—á–∏—Ç—å LoRA –¥–æ–æ–±—É—á–µ–Ω–∏–µ
    
    # –†–∞–Ω–≥ –º–∞—Ç—Ä–∏—Ü—ã LoRA (–º–µ–Ω—å—à–µ = —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
    # 16 –¥–ª—è 8GB, 32 –¥–ª—è 12GB+, 64 –¥–ª—è 24GB+
    lora_r: int = 16
    
    # Alpha –ø–∞—Ä–∞–º–µ—Ç—Ä LoRA (–æ–±—ã—á–Ω–æ lora_r * 2)
    lora_alpha: int = 32
    
    # Dropout –≤ LoRA (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
    lora_dropout: float = 0.1
    
    # ‚úÖ –ö–†–ò–¢–ò–ß–ù–û: –¶–ï–õ–ï–í–´–ï –ú–û–î–£–õ–ò –î–õ–Ø LORA
    # –£–±—Ä–∞–Ω 'lm_head' - —ç–∫–æ–Ω–æ–º–∏—Ç ~500MB VRAM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    lora_target_modules: List[str] = field(default_factory=lambda: [
        "q_proj", "k_proj", "v_proj", "o_proj",  # –í–Ω–∏–º–∞–Ω–∏–µ
        "gate_proj", "up_proj", "down_proj",    # FFN –±–ª–æ–∫–∏
        # ‚ùå –£–ë–†–ê–ù–û: "lm_head" (–Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è LoRA, —Ç—è–∂–µ–ª—ã–π)
    ])

@dataclass
class DocumentConfig:
    """
    –ù–ê–°–¢–†–û–ô–ö–ò –û–ë–†–ê–ë–û–¢–ö–ò –î–û–ö–£–ú–ï–ù–¢–û–í
    
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è IDE –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ .py, .java, .kt, .xml (Android Studio)
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ 50MB (–∑–∞—â–∏—Ç–∞ –æ—Ç –æ–≥—Ä–æ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤)
    """
    
    # –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    documents_path: str = "/app/data/documents"
    
    # –ü—É—Ç—å –∫ –∫—ç—à—É
    cache_path: str = "/app/data/cache"
    
    # ‚úÖ –†–ê–ó–ú–ï–† –ß–ê–ù–ö–û–í: 512 —Ç–æ–∫–µ–Ω–æ–≤
    # –ú–µ–Ω—å—à–∏–µ —á–∞–Ω–∫–∏ = –±—ã—Å—Ç—Ä–µ–µ –ø–æ–∏—Å–∫, –±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤
    # –î–ª—è –∫–æ–¥–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ 512 (–ø—Ä–∏–º–µ—Ä–Ω–æ 300-400 —Å—Ç—Ä–æ–∫)
    chunk_size: int = 512
    
    # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ (–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
    chunk_overlap: int = 50
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–∑–∞—â–∏—Ç–∞)
    max_file_size_mb: int = 50
    
    # ‚úÖ –ü–û–î–î–ï–†–ñ–ò–í–ê–ï–ú–´–ï –§–û–†–ú–ê–¢–´ (–¥–ª—è IDE)
    # –ò—Å—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã + –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    supported_formats: List[str] = field(default_factory=lambda: [
        # –ö–æ–¥
        ".py", ".java", ".kt", ".js", ".ts", ".cpp", ".c", ".h", ".cs", ".go", ".rs",
        # Android Studio
        ".xml", ".gradle", ".properties",
        # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
        ".pdf", ".docx", ".txt", ".md", ".html", ".rst"
    ])

@dataclass
class VectorConfig:
    """
    –ù–ê–°–¢–†–û–ô–ö–ò –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–´ (ChromaDB)
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    
    # –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç–∏)
    # –î–ª—è 8GB –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –≤–µ—Ä—Å–∏—é (MiniLM), –¥–ª—è 12GB+ –º–æ–∂–Ω–æ mpnet
    embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    # –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ ChromaDB
    collection_name: str = "ai-assistant-p104"
    
    # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤
    persist_directory: str = "/app/data/chroma"
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    search_k: int = 5
    
    # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (–æ—Ç 0.0 –¥–æ 1.0)
    similarity_threshold: float = 0.75

@dataclass
class IDEConfig:
    """
    –ù–ê–°–¢–†–û–ô–ö–ò –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –° IDE
    
    –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è VSCode –∏ Android Studio
    """
    
    # –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç–∞–º VSCode (–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤)
    vscode_projects_path: str = "/app/data/vscode_projects"
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–π –∫—ç—à —Ñ–∞–π–ª–æ–≤ (–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏)
    temp_cache_path: str = "/app/data/cache/ide"
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤
    max_scan_depth: int = 5
    
    # –§–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å
    ignore_patterns: List[str] = field(default_factory=lambda: [
        "*.class", "*.o", "*.so", "*.dll", "*.exe",  # –°–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        "*.log", "*.tmp", "*.temp", "*.cache",      # –í—Ä–µ–º–µ–Ω–Ω—ã–µ
        ".git/", ".svn/", ".idea/", ".vscode/",    # –°–ª—É–∂–µ–±–Ω—ã–µ
        "node_modules/", "dist/", "build/",         # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–±–æ—Ä–∫–∏
        "*.min.js", "*.min.css",                    # –ú–∏–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
    ])

class Config:
    """
    –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥ GPU
    """
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥—Å–µ–∫—Ü–∏–π
        self.model = ModelConfig()
        self.training = TrainingConfig()
        self.documents = DocumentConfig()
        self.vector = VectorConfig()
        self.ide = IDEConfig()
        
        # ‚úÖ –ê–í–¢–û–ù–ê–°–¢–†–û–ô–ö–ê –ü–û–î GPU
        self._validate_gpu_config()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        self._create_directories()
    
    def _validate_gpu_config(self):
        """
        –í–ê–õ–ò–î–ê–¶–ò–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –ü–û–î GPU
        
        –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è OOM
        """
        
        if not torch.cuda.is_available():
            warn("CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω! –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU (–±—É–¥–µ—Ç –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ)")
            self.model.device = "cpu"
            self.model.load_in_8bit = False
            self.training.fp16 = False
            return
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU 0
        gpu_properties = torch.cuda.get_device_properties(0)
        gpu_name = gpu_properties.name
        total_vram = gpu_properties.total_memory / (1024**3)  # –í GB
        
        log(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω GPU: {gpu_name} ({total_vram:.1f}GB)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ P104-100 (sm_61)
        if "P104-100" in gpu_name or total_vram < 9.0:
            log("üéØ –ü—Ä–∏–º–µ–Ω–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è 8GB VRAM (sm_61 Pascal)")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ 8-bit
            self.model.load_in_8bit = True
            self.model.load_in_4bit = False
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            self.model.max_new_tokens = min(self.model.max_new_tokens, 1024)
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π batch size
            self.training.per_device_train_batch_size = 1
            self.training.gradient_accumulation_steps = 8
            
            # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π LoRA —Ä–∞–Ω–≥
            self.training.lora_r = 16
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ VRAM < 8GB
            if total_vram < 8.0:
                warn(f"VRAM –º–µ–Ω—å—à–µ 8GB ({total_vram:.1f}GB)! –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è.")
        else:
            log("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω GPU —Å 12GB+ VRAM. –ú–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
            self.training.per_device_train_batch_size = 2
            self.training.lora_r = 32
    
    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        directories = [
            self.model.cache_dir,
            self.documents.documents_path,
            self.documents.cache_path,
            self.vector.persist_directory,
            self.ide.vscode_projects_path,
            self.ide.temp_cache_path,
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            log(f"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config = Config()