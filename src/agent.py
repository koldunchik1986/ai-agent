"""
–û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° AI-–ê–°–°–ò–°–¢–ï–ù–¢–ê

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- DocumentProcessor (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- ModelTrainer (–¥–æ–æ–±—É—á–µ–Ω–∏–µ)
- RAGEngine (–ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è)
- IDE integrations (VSCode, Android Studio)

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è 8GB VRAM:
- –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
"""

import os
import torch
from typing import Dict, List, Optional, Any
from pathlib import Path

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline
)

# –ò–º–ø–æ—Ä—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from config import config
from document_processor import DocumentProcessor
from model_trainer import ModelTrainer
from rag_engine import RAGEngine, RAGResponse

class AIAssistant:
    """
    AI-–ê–°–°–ò–°–¢–ï–ù–¢ –î–õ–Ø IDE –ò –î–û–ö–£–ú–ï–ù–¢–û–í
    
    –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    1. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (PDF/DOCX/TXT/HTML)
    2. –ß–∞—Ç —Å RAG (–ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º)
    3. –î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ–µ–∫—Ç–∞—Ö (LoRA)
    4. –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ –¥–ª—è IDE
    """
    
    def __init__(self, model_path: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        
        Args:
            model_path: –ü—É—Ç—å –∫ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        self.rag_engine = None
        self.doc_processor = None
        self.trainer = None
        
        # –¢–µ–∫—É—â–∏–π –ø—Ä–æ–µ–∫—Ç (–¥–ª—è IDE)
        self.current_project = None
        
        # ‚úÖ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω model_path - –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        self._load_model(model_path)
        
        print(f"‚úÖ AI-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –≥–æ—Ç–æ–≤")
        print(f"   –ú–æ–¥–µ–ª—å: {config.model.model_name}")
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {config.model.device}")
        print(f"   VRAM: {self._get_vram_usage():.2f}/{VRAM_LIMIT_GB}GB")
    
    def _get_vram_usage(self) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / (1024**3)
        return 0.0
    
    def _load_model(self, model_path: Optional[str] = None):
        """
        –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –° 8-BIT –ö–í–ê–ù–¢–ò–ó–ê–¶–ò–ï–ô
        
        –í–∞–∂–Ω–æ: –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
        """
        print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
        torch.cuda.empty_cache()
        
        # 8-bit –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            load_in_4bit=False,
        )
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path or config.model.model_name,
            cache_dir=config.model.cache_dir,
            trust_remote_code=config.model.trust_remote_code
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model_source = model_path or config.model.model_name
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_source,
            cache_dir=config.model.cache_dir,
            quantization_config=bnb_config,
            device_map={"": 0},  # –Ø–≤–Ω–æ –Ω–∞ GPU 0
            torch_dtype=torch.float16,
            trust_remote_code=True,
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ pipeline
        self.pipeline = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_p=config.model.top_p,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id,
        )
        
        vram_used = self._get_vram_usage()
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. VRAM: {vram_used:.2f}/{VRAM_LIMIT_GB}GB")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –¥–≤–∏–∂–∫–∞ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        self.rag_engine = RAGEngine(self.pipeline)
    
    # ============= –î–û–ë–ê–í–õ–ï–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–û–í =============
    
    def add_document(self, file_path: str) -> bool:
        """
        –î–û–ë–ê–í–ò–¢–¨ –î–û–ö–£–ú–ï–ù–¢ –í –ë–ê–ó–£ –ó–ù–ê–ù–ò–ô
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É (PDF/DOCX/TXT/HTML)
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
        """
        
        # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        if self.doc_processor is None:
            self.doc_processor = DocumentProcessor()
        
        try:
            return self.doc_processor.process_file(file_path)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return False
    
    def add_project(self, project_path: str) -> Dict[str, Any]:
        """
        –î–û–ë–ê–í–ò–¢–¨ –ü–†–û–ï–ö–¢ IDE (VSCode/Android Studio)
        
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ
        
        Args:
            project_path: –ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        print(f"\nüìÇ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞: {project_path}")
        
        if not os.path.exists(project_path):
            return {"success": False, "error": f"–ü—Ä–æ–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {project_path}"}
        
        if self.doc_processor is None:
            self.doc_processor = DocumentProcessor()
        
        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
        processed_files = 0
        errors = []
        
        for root, dirs, files in os.walk(project_path):
            # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            dirs[:] = [d for d in dirs if d not in config.ide.ignore_patterns]
            
            for file in files:
                file_path = os.path.join(root, file)
                ext = Path(file).suffix.lower()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
                if ext in config.documents.supported_formats:
                    try:
                        if self.doc_processor.process_file(file_path):
                            processed_files += 1
                    except Exception as e:
                        errors.append(f"{file_path}: {e}")
        
        print(f"‚úÖ –ü—Ä–æ–µ–∫—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {processed_files} —Ñ–∞–π–ª–æ–≤")
        
        return {
            "success": True,
            "processed_files": processed_files,
            "errors_count": len(errors),
            "errors": errors
        }
    
    # ============= –ß–ê–¢ –° RAG =============
    
    def chat(self, question: str) -> str:
        """
        –û–ë–©–ò–ô –ß–ê–¢ –° –í–û–ü–†–û–°–û–ú
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        """
        if self.rag_engine is None:
            raise RuntimeError("RAG –¥–≤–∏–∂–æ–∫ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        print(f"\nüí¨ –í–æ–ø—Ä–æ—Å: {question[:50]}...")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = self.rag_engine.ask(question)
        
        # –í—ã–≤–æ–¥ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
        if response.sources:
            print(f"üìã –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(response.sources)}")
        
        return response.answer
    
    def analyze_code_file(self, file_path: str) -> str:
        """
        –ê–ù–ê–õ–ò–ó –§–ê–ô–õ–ê –° –ò–°–•–û–î–ù–´–ú –ö–û–î–û–ú
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å IDE:
        1. –ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª
        2. –î–æ–±–∞–≤–ª—è–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–¥–∞
            
        Returns:
            –ê–Ω–∞–ª–∏–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        """
        if not os.path.exists(file_path):
            return f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"
        
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code_content = f.read()
        except:
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª (–æ—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏?)"
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
        max_tokens = config.model.max_file_tokens
        if len(code_content) > max_tokens * 4:  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
            code_content = code_content[:max_tokens * 4] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ –∏–∑-–∑–∞ —Ä–∞–∑–º–µ—Ä–∞)"
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞
        file_name = Path(file_path).name
        question = f"–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π —Ü–µ–π –∫–æ–¥ –∑ —Ñ–∞–π–ª—É {file_name}:\n\n{code_content}"
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        response = self.rag_engine.ask(question)
        
        return response.answer
    
    def debug_error(self, error_message: str, file_context: Optional[str] = None) -> str:
        """
        –û–¢–õ–ê–î–ö–ê –û–®–ò–ë–û–ö –í –ö–û–î–ï
        
        Args:
            error_message: –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ (—Å—Ç–µ–∫ —Ç—Ä–µ–π—Å)
            file_context: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é
        """
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –æ—à–∏–±–∫–æ–π
        prompt = f"""[INST] –ü–æ–º–æ–≥–∏ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ –ø–æ–º–∏–ª–∫—É –≤ –∫–æ–¥—ñ:

–ü–û–ú–ò–õ–ö–ê:
{error_message}

"""
        if file_context:
            prompt += f"–ö–û–î –î–ï –ü–†–û–ò–ó–û–®–õ–ê –û–®–ò–ë–ö–ê:\n{file_context[:500]}\n\n"
        
        prompt += "–ü–û–Ø–°–ù–ï–ù–ù–Ø –ü–†–ò–ß–ò–ù–ò –¢–ê –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: [/INST]"
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = self.pipeline(
            prompt,
            max_new_tokens=512,
            temperature=0.3,  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        )
        
        return response[0]['generated_text'] if response else "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
    
    def suggest_code(self, description: str, language: str = "python") -> str:
        """
        –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–û–î–ê –ü–û –û–ü–ò–°–ê–ù–ò–Æ
        
        Args:
            description: –û–ø–∏—Å–∞–Ω–∏–µ, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å
            language: –Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥
        """
        prompt = f"""[INST] –ù–∞–ø–∏—à–∏ –∫–æ–¥ –Ω–∞ {language}:

–ó–ê–ü–ò–¢:
{description}

–í–ò–ú–û–ì–ò:
1. –ö–æ–¥ –º–∞—î –±—É—Ç–∏ —Ä–æ–±–æ—á–∏–º —Ç–∞ –±–µ–∑–ø–µ—á–Ω–∏–º
2. –î–æ–¥–∞–π –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ
3. –í–∫–∞–∂–∏ –≤–∞–∂–ª–∏–≤—ñ –º–æ–º–µ–Ω—Ç–∏

–ö–û–î: [/INST]"""
        
        response = self.pipeline(
            prompt,
            max_new_tokens=config.model.max_new_tokens,
            temperature=0.7,
        )
        
        return response[0]['generated_text'] if response else ""
    
    # ============= –î–û–û–ë–£–ß–ï–ù–ò–ï =============
    
    def train_on_documents(self, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        –î–û–û–ë–£–ß–ï–ù–ò–ï –ù–ê –ó–ê–ì–†–£–ñ–ï–ù–ù–´–• –î–û–ö–£–ú–ï–ù–¢–ê–•
        
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å LoRA fine-tuning –Ω–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ
        
        Args:
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            
        Returns:
            –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
        """
        
        # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        if self.trainer is None:
            self.trainer = ModelTrainer()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        if self.doc_processor is None:
            self.doc_processor = DocumentProcessor()
        
        print("\nüìö –°–±–æ—Ä —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        
        from document_processor import ProcessedChunk
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ ChromaDB
        collection = self.doc_processor.vectorstore._collection
        all_data = collection.get()
        
        if not all_data['documents']:
            return {"success": False, "error": "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"}
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ ProcessedChunk
        chunks = []
        for idx, doc_content in enumerate(all_data['documents']):
            metadata = all_data['metadatas'][idx]
            
            chunk = ProcessedChunk(
                content=doc_content,
                metadata=metadata,
                vector_id=all_data['ids'][idx] if 'ids' in all_data else None
            )
            chunks.append(chunk)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        return self.trainer.train(chunks, output_dir)
    
    # ============= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ =============
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        return {
            "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
            "vram_used_gb": self._get_vram_usage(),
            "vram_total_gb": VRAM_LIMIT_GB,
            "model_loaded": self.model is not None,
            "rag_ready": self.rag_engine is not None,
            "documents_db": self.doc_processor.get_stats()["vectors_in_db"] if self.doc_processor else 0,
        }
    
    def clear_memory(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"üßπ GPU –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {self._get_vram_usage():.2f}GB")