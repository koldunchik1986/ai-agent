"""
–û–ë–†–ê–ë–û–¢–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í –î–õ–Ø AI-–ê–°–°–ò–°–¢–ï–ù–¢–ê

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ PDF/DOCX/TXT/HTML
- –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥ 8GB VRAM (batch processing)
"""

import os
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Generator, Any
from dataclasses import dataclass

import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter

# –ò–º–ø–æ—Ä—Ç_loaders
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    UnstructuredHTMLLoader
)

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

from config import config

@dataclass
class ProcessedChunk:
    """
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞
    
    Attributes:
        content: –¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞
        metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, —Ç–∏–ø –∏ —Ç.–¥.)
        vector_id: ID –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ
        confidence: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    content: str
    metadata: Dict[str, Any]
    vector_id: Optional[str] = None
    confidence: float = 1.0

class DocumentProcessor:
    """
    –ü–†–û–¶–ï–°–°–û–† –î–û–ö–£–ú–ï–ù–¢–û–í
    
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
    1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    2. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ (chunking)
    3. –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ ChromaDB
    """
    
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = config
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è text splitter
        # –†–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.documents.chunk_size,
            chunk_overlap=self.config.documents.chunk_overlap,
            length_function=len,
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º, –ø–æ—Ç–æ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        
        # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ª–µ–Ω–∏–≤–∞—è)
        # –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        self._embeddings = None
        self._vectorstore = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "files_processed": 0,
            "chunks_created": 0,
            "vectors_stored": 0,
            "total_size_mb": 0.0
        }
    
    @property
    def embeddings(self) -> HuggingFaceEmbeddings:
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ)"""
        if self._embeddings is None:
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
            model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
            
            # –î–ª—è 8GB –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å, –¥–ª—è 12GB+ - –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                if gpu_memory < 9:
                    # ‚úÖ –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM (~200MB)
                    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                else:
                    # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è 12GB+ (~700MB)
                    model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
            else:
                # –î–ª—è CPU (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ!)
                model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            
            self._embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs
            )
            print(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: {model_name}")
        
        return self._embeddings
    
    @property
    def vectorstore(self) -> Chroma:
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã (—Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ)"""
        if self._vectorstore is None:
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            persist_dir = Path(self.config.vector.persist_directory)
            persist_dir.mkdir(parents=True, exist_ok=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –∫–æ–ª–ª–µ–∫—Ü–∏—è
            if (persist_dir / "chroma.sqlite3").exists():
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                self._vectorstore = Chroma(
                    persist_directory=str(persist_dir),
                    embedding_function=self.embeddings,
                    collection_name=self.config.vector.collection_name
                )
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ ({persist_dir})")
            else:
                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                self._vectorstore = Chroma(
                    collection_name=self.config.vector.collection_name,
                    persist_directory=str(persist_dir),
                    embedding_function=self.embeddings
                )
                print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ ({persist_dir})")
        
        return self._vectorstore
    
    def load_document(self, file_path: str) -> Optional[List[Any]]:
        """
        –ó–ê–ì–†–£–ó–ö–ê –î–û–ö–£–ú–ï–ù–¢–ê
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:
        - PDF: PyMuPDF extraction
        - DOCX: python-docx –ø–∞—Ä—Å–µ—Ä
        - TXT: –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
        - HTML: BeautifulSoup —Ä–∞–∑–±–æ—Ä
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            List of Document objects –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        path = Path(file_path)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        if not path.exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        file_size_mb = path.stat().st_size / (1024 * 1024)
        if file_size_mb > self.config.documents.max_file_size_mb:
            print(f"‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file_size_mb:.1f}MB > {self.config.documents.max_file_size_mb}MB)")
            # TODO: –î–æ–±–∞–≤–∏—Ç—å –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        
        # –í—ã–±–æ—Ä –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        ext = path.suffix.lower()
        
        try:
            if ext == '.pdf':
                loader = PyPDFLoader(str(path))
            elif ext == '.docx':
                loader = Docx2txtLoader(str(path))
            elif ext == '.txt':
                loader = TextLoader(str(path), encoding='utf-8')
            elif ext == '.html':
                loader = UnstructuredHTMLLoader(str(path))
            else:
                print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {ext}")
                return None
            
            # ‚úÖ –ü–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å —Å—Ä–∞–∑—É)
            documents = loader.load()
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {path.name} ({len(documents)} —Å—Ç—Ä–∞–Ω–∏—Ü/—á–∞—Å—Ç–µ–π)")
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats["files_processed"] += 1
            self.stats["total_size_mb"] += file_size_mb
            
            return documents
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path.name}: {e}")
            return None
    
    def chunk_document(self, documents: List[Any]) -> List[ProcessedChunk]:
        """
        –†–ê–ó–ë–ò–ï–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–ê –ù–ê –ß–ê–ù–ö–ò
        
        –ü—Ä–æ—Ü–µ—Å—Å:
        1. –ë–µ—Ä–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        2. –†–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ chunk_size (512 —Ç–æ–∫–µ–Ω–æ–≤)
        3. –î–æ–±–∞–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
        
        Args:
            documents: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ ProcessedChunk
        """
        if not documents:
            return []
        
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.text_splitter.split_documents(documents)
        
        processed_chunks = []
        for idx, chunk in enumerate(chunks):
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è —á–∞–Ω–∫–∞
            content_hash = hashlib.md5(chunk.page_content.encode()).hexdigest()[:8]
            
            chunk_metadata = {
                "chunk_id": f"{path.stem}_{idx}_{content_hash}",
                "chunk_index": idx,
                "total_chunks": len(chunks),
                "source_file": str(path),
                "file_name": path.name,
                "file_ext": path.suffix,
                "processing_timestamp": torch.datetime.now().isoformat(),
                "confidence": 0.95  # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            }
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            if hasattr(chunk, 'metadata'):
                chunk_metadata.update(chunk.metadata)
            
            processed_chunk = ProcessedChunk(
                content=chunk.page_content,
                metadata=chunk_metadata
            )
            
            processed_chunks.append(processed_chunk)
        
        print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(processed_chunks)} —á–∞–Ω–∫–æ–≤")
        self.stats["chunks_created"] += len(processed_chunks)
        
        return processed_chunks
    
    def create_embeddings(self, chunks: List[ProcessedChunk]) -> List[str]:
        """
        –°–û–ó–î–ê–ù–ò–ï –í–ï–ö–¢–û–†–ù–´–• –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
        
        Process:
        1. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä—ã
        2. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ChromaDB
        3. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ID –≤–µ–∫—Ç–æ—Ä–æ–≤
        
        ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥ 8GB: Batch processing —Å –æ—á–∏—Å—Ç–∫–æ–π –∫—ç—à–∞
        """
        if not chunks:
            return []
        
        # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (batch_size=10 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
        batch_size = 10
        vector_ids = []
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            batch_texts = [chunk.content for chunk in batch]
            batch_metadatas = [chunk.metadata for chunk in batch]
            batch_ids = [chunk.metadata["chunk_id"] for chunk in batch]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            ids = self.vectorstore.add_texts(
                texts=batch_texts,
                metadatas=batch_metadatas,
                ids=batch_ids
            )
            
            vector_ids.extend(ids)
            
            # ‚úÖ –û–ß–ò–°–¢–ö–ê –ö–≠–®–ê GPU –ü–û–°–õ–ï –ö–ê–ñ–î–û–ì–û –ë–ê–¢–ß–ê
            # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print(f"  –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–æ {len(ids)} —á–∞–Ω–∫–æ–≤...")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ –¥–∏—Å–∫ (persist)
        self.vectorstore.persist()
        print(f"‚úÖ –í–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ChromaDB")
        
        self.stats["vectors_stored"] += len(vector_ids)
        
        return vector_ids
    
    def process_file(self, file_path: str) -> bool:
        """
        –ü–û–õ–ù–´–ô –ü–†–û–¶–ï–°–° –û–ë–†–ê–ë–û–¢–ö–ò –§–ê–ô–õ–ê
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        try:
            print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path}")
            
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞
            documents = self.load_document(file_path)
            if not documents:
                return False
            
            # 2. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.chunk_document(documents)
            if not chunks:
                return False
            
            # 3. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
            vector_ids = self.create_embeddings(chunks)
            
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(chunks)} —á–∞–Ω–∫–æ–≤ ‚Üí {len(vector_ids)} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")
            return False
    
    def search_similar(self, query: str, k: Optional[int] = None, 
                      score_threshold: Optional[float] = None) -> List[Dict]:
        """
        –ü–û–ò–°–ö –ü–û–•–û–ñ–ò–• –î–û–ö–£–ú–ï–ù–¢–û–í
        
        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            score_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not query or not query.strip():
            return []
        
        if k is None:
            k = self.config.vector.search_k
        
        if score_threshold is None:
            score_threshold = self.config.vector.similarity_threshold
        
        try:
            # –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ
            # similarity_search_with_score –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (document, score)
            results = self.vectorstore.similarity_search_with_score(
                query,
                k=k,
                score_threshold=score_threshold
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": score,  # –ú–µ–Ω—å—à–µ = –ª—É—á—à–µ (0=–∏–¥–µ–Ω—Ç–∏—á–Ω–æ)
                    "relevance": 1.0 - score  # –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —á—Ç–µ–Ω–∏—è
                })
            
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(formatted_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            return formatted_results
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            collection_count = self.vectorstore._collection.count()
        except:
            collection_count = 0
        
        self.stats["vectors_in_db"] = collection_count
        
        return self.stats
    
    def delete_collection(self):
        """–£–¥–∞–ª–∏—Ç—å –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é (–¥–ª—è —Å–±—Ä–æ—Å–∞)"""
        print("‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ChromaDB...")
        self.vectorstore.delete_collection()
        self.vectorstore.persist()
        print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞")
        
        # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats = {
            "files_processed": 0,
            "chunks_created": 0,
            "vectors_stored": 0,
            "total_size_mb": 0.0
        }