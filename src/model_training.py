"""
–ú–û–î–£–õ–¨ –î–û–û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò –ß–ï–†–ï–ó LoRA

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è 8GB VRAM:
- 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
- Paged AdamW 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
- Batch size = 1 —Å gradient accumulation
- LoRA –±–µ–∑ lm_head (—ç–∫–æ–Ω–æ–º–∏—è 500MB)
- –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
"""

import os
import json
import torch
from datetime import datetime
from typing import List, Dict, Optional, Any
from pathlib import Path

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)

from datasets import Dataset

from config import config
from document_processor import ProcessedChunk

# –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
from tqdm import tqdm
for epoch in tqdm(range(epochs), desc="–û–±—É—á–µ–Ω–∏–µ"):

class ModelTrainer:
    """
    –ö–õ–ê–°–° –î–õ–Ø –î–û–û–ë–£–ß–ï–ù–ò–Ø MISTRAL-7B –ß–ï–†–ï–ó LoRA
    
    –û—Å–Ω–æ–≤–Ω—ã–µ —à–∞–≥–∏:
    1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
    2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
    3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    4. –ó–∞–ø—É—Å–∫ fine-tuning
    5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.peft_config = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è)
        self._init_tokenizer()
    
    def _init_tokenizer(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.model.model_name,
            cache_dir=config.model.cache_dir,
            trust_remote_code=config.model.trust_remote_code
        )
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ pad_token –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        # –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è batching
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    def load_model_for_training(self):
        """
        –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –° 8-BIT –ö–í–ê–ù–¢–ò–ó–ê–¶–ò–ï–ô
        
        –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è 8GB VRAM:
        - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç BitsAndBytesConfig –¥–ª—è 8-bit –∑–∞–≥—Ä—É–∑–∫–∏
        - –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç device_map –Ω–∞ GPU 0
        - –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è k-bit –æ–±—É—á–µ–Ω–∏—è
        
        VRAM usage: ~6.5GB –∏–∑ 8GB
        """
        print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π...")
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π (–≤–∞–∂–Ω–æ!)
        torch.cuda.empty_cache()
        
        # ‚úÖ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø 8-BIT –ö–í–ê–ù–¢–ò–ó–ê–¶–ò–ò
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å 14GB –º–æ–¥–µ–ª—å –≤ 8GB VRAM
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,           # –í–∫–ª—é—á–∏—Ç—å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
            load_in_4bit=False,          # –û—Ç–∫–ª—é—á–∏—Ç—å 4-bit (–∫–∞—á–µ—Å—Ç–≤–æ —Ö—É–∂–µ)
            bnb_4bit_compute_dtype=torch.float16,  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        )
        
        # ‚úÖ –Ø–í–ù–û–ï –ù–ê–ó–ù–ê–ß–ï–ù–ò–ï –ù–ê GPU 0
        # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
        device_map = {"": 0}
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.model = AutoModelForCausalLM.from_pretrained(
            config.model.model_name,
            cache_dir=config.model.cache_dir,
            quantization_config=bnb_config,
            device_map=device_map,
            torch_dtype=torch.float16,  # Mixed precision
            trust_remote_code=config.model.trust_remote_code,
        )
        
        # ‚úÖ –ü–û–î–ì–û–¢–û–í–ö–ê –î–õ–Ø K-BIT –û–ë–£–ß–ï–ù–ò–ù–ì–ê
        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º LoRA
        self.model = prepare_model_for_kbit_training(self.model)
        
        # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞–º—è—Ç–∏
        vram_used = torch.cuda.memory_allocated() / (1024**3)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ VRAM: {vram_used:.2f}/{VRAM_LIMIT_GB}GB")
        
        if vram_used > SAFE_VRAM_USAGE_GB:
            print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ >{SAFE_VRAM_USAGE_GB}GB! –û–û–ú –≤–æ–∑–º–æ–∂–µ–Ω –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.")
    
    def setup_lora(self):
        """
        –ù–ê–°–¢–†–û–ô–ö–ê LoRA –ê–î–ê–ü–¢–ï–†–ê
        
        LoRA (Low-Rank Adaptation) –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å
        –ù–ï —Ç—Ä–æ–≥–∞—è –≤—Å–µ 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ —Ç–æ–ª—å–∫–æ –Ω–µ–±–æ–ª—å—à–∏–µ –∞–¥–∞–ø—Ç–µ—Ä—ã
        
        –≠–∫–æ–Ω–æ–º–∏—è: –í–º–µ—Å—Ç–æ 14GB –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ fine-tuning –∏—Å–ø–æ–ª—å–∑—É–µ–º ~500MB
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - lora_r=16: –†–∞–Ω–≥ –º–∞—Ç—Ä–∏—Ü—ã (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –≤—ã—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏)
        - lora_alpha=32: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–æ–±—ã—á–Ω–æ lora_r * 2)
        - target_modules: –ö–∞–∫–∏–µ —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ –¥–æ–æ–±—É—á–∞—Ç—å
        """
        print("\nüéØ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞...")
        
        # ‚úÖ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø LORA
        # task_type=CAUSAL_LM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=config.training.lora_r,                    # 16 –¥–ª—è 8GB
            lora_alpha=config.training.lora_alpha,       # 32
            lora_dropout=config.training.lora_dropout,   # 0.1
            target_modules=config.training.lora_target_modules,
            # –¶–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏ –±–µ–∑ lm_head (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
            bias="none",  # –ù–µ —Ç—Ä–æ–≥–∞–µ–º bias –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        )
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∫ –º–æ–¥–µ–ª–∏
        self.model = get_peft_model(self.model, self.peft_config)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
        # trainable params: 16,777,216 || all params: 7,110,350,848
        self.model.print_trainable_parameters()
    
    def format_training_data(self, chunks: List[ProcessedChunk]) -> Dataset:
        """
        –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï –î–ê–ù–ù–´–• –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø
        
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç ProcessedChunk –≤ —Ñ–æ—Ä–º–∞—Ç, –ø–æ–Ω—è—Ç–Ω—ã–π HuggingFace Trainer
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            
        Returns:
            Dataset –≥–æ—Ç–æ–≤—ã–π –∫ –æ–±—É—á–µ–Ω–∏—é
        """
        if not chunks:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        print(f"\nüìä –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len(chunks)} —á–∞–Ω–∫–æ–≤...")
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è training example
        def create_instruction(chunk: ProcessedChunk) -> str:
            """
            –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Mistral
            
            –§–æ—Ä–º–∞—Ç: <s>[INST] –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è [/INST] –û—Ç–≤–µ—Ç</s>
            """
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            file_ext = chunk.metadata.get("file_ext", "–¥–æ–∫—É–º–µ–Ω—Ç")
            file_name = chunk.metadata.get("file_name", "—Ñ–∞–π–ª")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
            if file_ext in ['.py', '.java', '.kt', '.js']:
                # –î–ª—è –∫–æ–¥–∞
                instruction = (
                    f"–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π —Ü–µ–π –∫–æ–¥ –∑ —Ñ–∞–π–ª—É {file_name} —Ç–∞ –ø–æ—è—Å–Ω–∏ –π–æ–≥–æ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª:\n\n"
                    f"{chunk.content[:800]}"  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
                )
            elif file_ext in ['.docx', '.pdf']:
                # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
                instruction = (
                    f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {file_name}, –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–π –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è:\n\n"
                    f"–ó–º—ñ—Å—Ç: {chunk.content[:500]}\n\n"
                    f"–ü–∏—Ç–∞–Ω–Ω—è:"
                )
            else:
                # –î–ª—è –æ–±—â–∏—Ö —Ñ–∞–π–ª–æ–≤
                instruction = (
                    f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑ —Ñ–∞–π–ª—É {file_name}:\n\n"
                    f"{chunk.content[:600]}"
                )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç–∏–ª—å Mistral
            return f"<s>[INST] {instruction.strip()} [/INST]"
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        formatted_data = {"text": []}
        for chunk in chunks:
            formatted_data["text"].append(create_instruction(chunk))
        
        # –°–æ–∑–¥–∞–Ω–∏–µ HuggingFace Dataset
        dataset = Dataset.from_dict(formatted_data)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(dataset)} –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        return dataset
    
    def tokenize_dataset(self, dataset: Dataset) -> Dataset:
        """
        –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–ê
        
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã (—á–∏—Å–ª–æ–≤—ã–µ ID)
        
        Args:
            dataset: –î–∞—Ç–∞—Å–µ—Ç —Å —Ç–µ–∫—Å—Ç–æ–º
            
        Returns:
            –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        """
        print("\nüîÑ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        def tokenize_function(examples):
            """
            –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è batched processing
            
            –í–∞–∂–Ω–æ: truncation=True - –æ—Ç—Å–µ–∫–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                   padding=False - –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º padding (—Å–¥–µ–ª–∞–µ—Ç collator)
            """
            return self.tokenizer(
                examples["text"],
                truncation=True,  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ! –ò–Ω–∞—á–µ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
                padding=False,      # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º padding –∑–¥–µ—Å—å
                max_length=config.model.max_new_tokens,
                return_overflowing_tokens=False,
            )
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (batched –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
            batch_size=10  # –ù–µ–±–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        )
        
        print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(tokenized_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        return tokenized_dataset
    
    def prepare_training_data(self, chunks: List[ProcessedChunk]) -> tuple[Dataset, Dataset]:
        """
        –ü–û–î–ì–û–¢–û–í–ö–ê –û–ë–£–ß–ê–Æ–©–ò–• –î–ê–ù–ù–´–•
        
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω:
        1. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ ProcessedChunk
            
        Returns:
            (train_dataset, eval_dataset)
        """
        if not chunks:
            raise ValueError("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        print(f"\nüìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        
        # 1. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        dataset = self.format_training_data(chunks)
        
        # 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokenized = self.tokenize_dataset(dataset)
        
        # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation (90/10)
        # stratify=False - –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫
        split = tokenized.train_test_split(
            test_size=0.1,  # 10% –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é
            seed=42,        # –§–∏–∫—Å–∏—Ä—É–µ–º random seed
            stratify=False
        )
        
        train_dataset = split["train"]
        eval_dataset = split["test"]
        
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω: {len(train_dataset)} train, {len(eval_dataset)} eval")
        
        return train_dataset, eval_dataset
    
    def create_trainer(self, train_dataset: Dataset, eval_dataset: Dataset) -> Trainer:
        """
        –°–û–ó–î–ê–ù–ò–ï –¢–†–ï–ù–ï–†–ê
        
        Trainer —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ü–∏–∫–ª–æ–º –æ–±—É—á–µ–Ω–∏—è:
        - Forward pass
        - Loss calculation
        - Backward pass
        - Optimizer step
        - Logging & saving
        
        Args:
            train_dataset: –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            eval_dataset: –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            Trainer –≥–æ—Ç–æ–≤—ã–π –∫ –∑–∞–ø—É—Å–∫—É
        """
        print("\nüéØ –°–æ–∑–¥–∞–Ω–∏–µ Trainer...")
        
        # ‚úÖ –ù–ê–°–¢–†–û–ô–ö–ò –û–ë–£–ß–ï–ù–ò–Ø
        # –°–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é training_output
        training_args = TrainingArguments(
            # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
            output_dir=f"{config.model.cache_dir}/training_output",
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            num_train_epochs=config.training.num_train_epochs,
            
            # ‚úÖ –ü–ê–†–ê–ú–ï–¢–†–´ –ë–ê–¢–ß–ï–ô (–ö–†–ò–¢–ò–ß–ù–û)
            per_device_train_batch_size=config.training.per_device_train_batch_size,  # 1 –¥–ª—è 8GB
            per_device_eval_batch_size=config.training.per_device_eval_batch_size,    # 1
            gradient_accumulation_steps=config.training.gradient_accumulation_steps,  # 8
            
            # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            learning_rate=config.training.learning_rate,
            weight_decay=config.training.weight_decay,
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (8-bit –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
            optim=config.training.optim,
            
            # –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏
            lr_scheduler_type=config.training.lr_scheduler_type,
            
            # –®–∞–≥–∏ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞
            warmup_steps=config.training.warmup_steps,
            
            # Mixed precision (—É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ GPU)
            fp16=config.training.fp16,
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            logging_steps=config.training.logging_steps,
            save_steps=config.training.save_steps,
            evaluation_strategy="steps",
            eval_steps=config.training.eval_steps,
            save_total_limit=config.training.save_total_limit,
            
            # ‚úÖ –û–¢–ö–õ–Æ–ß–ï–ù–ò–ï –í–ï–ó–ë/TensorBoard (–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –≤ Docker)
            report_to=None,
            
            # ‚úÖ –≠–ö–û–ù–û–ú–ò–Ø –ü–ê–ú–Ø–¢–ò
            dataloader_drop_last=True,      # –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–µ–ø–æ–ª–Ω—ã–π –±–∞—Ç—á
            dataloader_pin_memory=False,    # –û—Ç–∫–ª—é—á–∞–µ–º pin_memory (—ç–∫–æ–Ω–æ–º–∏—è)
            remove_unused_columns=False,    # –ù–µ —É–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏
            load_best_model_at_end=True,    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            metric_for_best_model="eval_loss",
            greater_is_better=False,
        )
        
        # Data Collator
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç padding –∫ –±–∞—Ç—á–∞–º
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # Causal LM (–Ω–µ masked)
        )
        
        # ‚úÖ –°–û–ó–î–ê–ù–ò–ï TRAINER
        # –û—Å–Ω–æ–≤–Ω–æ–π –æ–±—ä–µ–∫—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        trainer = Trainer(
            model=self.model,              # –ú–æ–¥–µ–ª—å —Å LoRA
            args=training_args,            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            train_dataset=train_dataset,   # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            eval_dataset=eval_dataset,     # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            data_collator=data_collator,   # Collator –¥–ª—è padding
            tokenizer=self.tokenizer,      # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–∞
        )
        
        print("‚úÖ Trainer —Å–æ–∑–¥–∞–Ω")
        
        return trainer
    
    def train(self, chunks: List[ProcessedChunk], output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î –û–ë–£–ß–ï–ù–ò–Ø
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –µ—Å–ª–∏ None)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        if not chunks:
            raise ValueError("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        print("\n" + "="*60)
        print("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø LoRA")
        print("="*60)
        
        start_time = datetime.now()
        
        # ‚úÖ 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_dataset, eval_dataset = self.prepare_training_data(chunks)
        
        # ‚úÖ 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.load_model_for_training()
        
        # ‚úÖ 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
        self.setup_lora()
        
        # ‚úÖ 4. –°–æ–∑–¥–∞–Ω–∏–µ Trainer
        trainer = self.create_trainer(train_dataset, eval_dataset)
        
        # ‚úÖ 5. –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø
        print("\nüéØ –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è fine-tuning...")
        print(f"–ü—Ä–∏–º–µ—Ä–æ–≤: {len(train_dataset)} train, {len(eval_dataset)} eval")
        print(f"–≠–ø–æ—Ö–∏: {config.training.num_train_epochs}")
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {output_dir or 'auto'}\n")
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        train_result = trainer.train()
        
        # ‚úÖ 6. –û–¶–ï–ù–ö–ê
        print("\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
        eval_result = trainer.evaluate()
        
        # ‚úÖ 7. –°–û–•–†–ê–ù–ï–ù–ò–ï
        if output_dir is None:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"{config.model.cache_dir}/lora_finetuned_{timestamp}"
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {output_dir}...")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
        trainer.model.save_pretrained(output_dir)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer.save_pretrained(output_dir)
        
        # ‚úÖ 8. –ú–ï–¢–ê–î–ê–ù–ù–´–ï
        metadata = {
            "training_timestamp": datetime.now().isoformat(),
            "model_name": config.model.model_name,
            "gpu_used": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
            "training_duration": str(datetime.now() - start_time),
            "output_dir": output_dir,
            "parameters": {
                "epochs": config.training.num_train_epochs,
                "learning_rate": config.training.learning_rate,
                "batch_size": config.training.per_device_train_batch_size,
                "gradient_accumulation": config.training.gradient_accumulation_steps,
                "lora_r": config.training.lora_r,
            },
            "dataset": {
                "train_samples": len(train_dataset),
                "eval_samples": len(eval_dataset),
                "total_chunks": len(chunks),
            },
            "results": {
                "train_loss": float(train_result.training_loss) if train_result.training_loss else None,
                "eval_loss": eval_result.get("eval_loss"),
                "train_runtime": train_result.training_time,
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ JSON
        metadata_path = os.path.join(output_dir, "training_metadata.json")
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
        
        # ‚úÖ 9. –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò
        # –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏
        del self.model
        torch.cuda.empty_cache()
        
        duration = datetime.now() - start_time
        
        print("\n" + "="*60)
        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print("="*60)
        print(f"‚è±Ô∏è  –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration}")
        print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_dir}")
        print(f"üìâ –ü–æ—Ç–µ—Ä–∏: train={metadata['results']['train_loss']:.4f}, eval={metadata['results']['eval_loss']:.4f}")
        
        return metadata
    
    def load_finetuned_model(self, model_path: str):
        """
        –ó–ê–ì–†–£–ó–ö–ê –î–û–û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò
        
        Args:
            model_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–º
            
        Returns:
            –ú–æ–¥–µ–ª—å —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º LoRA
        """
        print(f"\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        torch.cuda.empty_cache()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        base_model = AutoModelForCausalLM.from_pretrained(
            config.model.model_name,
            cache_dir=config.model.cache_dir,
            quantization_config=BitsAndBytesConfig(load_in_8bit=True),
            device_map={"": 0},
            torch_dtype=torch.float16,
        )
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
        # –ú–æ–¥–µ–ª—å = base + LoRA weights
        self.model = get_peft_model(base_model, model_path)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        print("‚úÖ –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        return self.model